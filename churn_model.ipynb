{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa72cf89-4f4a-4ec4-84a1-a35dea361b90",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8ff44887-649e-4008-a878-3d8cd34ed8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Modeling + evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, roc_auc_score, average_precision_score,\n",
    "    confusion_matrix, RocCurveDisplay, PrecisionRecallDisplay\n",
    ")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38433388-0806-493c-8b4f-72cf3809d0b9",
   "metadata": {},
   "source": [
    "# Set up helper functions and constants\n",
    "* Defines constants and small helper functions that will be reused later when cleaning and transforming raw tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6330e19c-88b3-4a9a-a97a-0817e3e4d903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets reference date for calculating things like account age\n",
    "DATA_CUTOFF = pd.to_datetime(\"2025-07-01\")  # end of data window (per brief)\n",
    "\n",
    "# Set random seed for reproducibility.\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Safe wrapper around pd.read_csv\n",
    "def safe_read(name):\n",
    "    p = Path(name)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing file: {name} (expected in {Path.cwd()})\")\n",
    "    return pd.read_csv(p)\n",
    "\n",
    "# Helper for session aggregation\n",
    "# Takes user’s first session and last session, and computes how many weeks they’ve been active\n",
    "def weeks_between(min_ts, max_ts):\n",
    "    if pd.isna(min_ts) or pd.isna(max_ts) or max_ts < min_ts:\n",
    "        return np.nan\n",
    "    return max(1.0, (max_ts - min_ts).days / 7.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935ac233-650a-4ab1-828d-9823b0e152b3",
   "metadata": {},
   "source": [
    "# Read the raw CSVs and standardize data types\n",
    "* Loads the raw data (users, sessions, events, billing)\n",
    "* Standardizes date/time columns into a consistent datetime format\n",
    "* Prepares the data for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0d386eb4-bb9b-4d56-a76b-d9beaed6128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads each dataset into a pandas DataFrame\n",
    "users   = safe_read(\"users.csv\") # safe_read() gives error message if file is missing\n",
    "sessions= safe_read(\"sessions.csv\")\n",
    "events  = safe_read(\"events.csv\")\n",
    "billing = safe_read(\"billing.csv\")\n",
    "\n",
    "# Converts the signup_date column into a datetime object.\n",
    "for col in [\"signup_date\"]:\n",
    "    if col in users.columns:\n",
    "        users[col] = pd.to_datetime(users[col], errors=\"coerce\") # errors=\"coerce\" means if a row has something invalid, doesn't crash\n",
    "\n",
    "# Ensure session_start and session_end are proper datetimes\n",
    "# Allows us to compute session length, first session date, and last session date per user\n",
    "for col in [\"session_start\", \"session_end\"]:\n",
    "    if col in sessions.columns:\n",
    "        sessions[col] = pd.to_datetime(sessions[col], errors=\"coerce\")\n",
    "\n",
    "# Converts the event timestamp (ts) into datetime\n",
    "if \"ts\" in events.columns:\n",
    "    events[\"ts\"] = pd.to_datetime(events[\"ts\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55d4860-4bbd-40b6-b8cf-d18d99052d2a",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "* Turns raw logs → engineered features (numeric + categorical inputs).\n",
    "* Organizes them into per-user rows (so every user has one feature vector)\n",
    "* Creates the foundation for the churn prediction dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673aeb6b-5785-4256-8470-cbc25d188def",
   "metadata": {},
   "source": [
    "### Users\n",
    "* Anchor” table — one row per user\n",
    "* Contains demographics (plan_tier, company_size, region), acquisition channel and churn label (churned_30d, churned_90d)\n",
    "* Gives who the user is and provides the target variable (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8beef732-7e7b-44d7-aa6f-2a89a0da2c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose 90-day churn horizon\n",
    "label_col = \"churned_90d\" if \"churned_90d\" in users.columns else \"churned_30d\" # If 90 days not in users, choose 30 days\n",
    "if label_col not in users.columns:\n",
    "    raise ValueError(\"Neither churned_90d nor churned_30d exists in users.csv\") # Error message\n",
    "\n",
    "user_feats = users.copy()\n",
    "\n",
    "# Calculate account age (days since signup to data cutoff)\n",
    "if \"signup_date\" in user_feats.columns:\n",
    "    user_feats[\"account_age_days\"] = (DATA_CUTOFF - user_feats[\"signup_date\"]).dt.days\n",
    "    user_feats.drop(columns=[\"signup_date\"], inplace=True, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2219beda-9d5e-4156-bc23-32a1033b7677",
   "metadata": {},
   "source": [
    "## Sessions\n",
    "* Raw sessions logs → aggregated into behavioral metrics like sessions_per_week, avg_session_length, device_variety\n",
    "* Captures how much the user engages with Atlassian products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0f8776cb-2217-4895-8803-1e4dc9713805",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sessions) > 0:\n",
    "    ses = sessions.copy()\n",
    "    # Safety: if session_length_sec missing, compute from start/end\n",
    "    if \"session_length_sec\" not in ses.columns and {\"session_start\", \"session_end\"} <= set(ses.columns):\n",
    "        ses[\"session_length_sec\"] = (ses[\"session_end\"] - ses[\"session_start\"]).dt.total_seconds()\n",
    "\n",
    "    # per-user aggregates\n",
    "    ses_agg = ses.groupby(\"user_id\").agg(\n",
    "        sessions_count=(\"session_id\", \"nunique\"),\n",
    "        avg_session_length=(\"session_length_sec\", \"mean\"),\n",
    "        p75_session_length=(\"session_length_sec\", lambda x: np.nanpercentile(x, 75)),\n",
    "        first_session=(\"session_start\", \"min\"),\n",
    "        last_session=(\"session_end\", \"max\"),\n",
    "        device_variety=(\"device\", pd.Series.nunique) if \"device\" in ses.columns else (\"session_id\", \"count\"),\n",
    "        os_variety=(\"os\", pd.Series.nunique) if \"os\" in ses.columns else (\"session_id\", \"count\"),\n",
    "        app_version_variety=(\"app_version\", pd.Series.nunique) if \"app_version\" in ses.columns else (\"session_id\", \"count\"),\n",
    "        country_variety=(\"country\", pd.Series.nunique) if \"country\" in ses.columns else (\"session_id\", \"count\")\n",
    "    )\n",
    "\n",
    "    # weeks active + sessions per week\n",
    "    ses_agg[\"weeks_active\"] = (ses_agg.apply(lambda r: weeks_between(r[\"first_session\"], r[\"last_session\"]), axis=1))\n",
    "    ses_agg[\"sessions_per_week\"] = ses_agg[\"sessions_count\"] / ses_agg[\"weeks_active\"]\n",
    "\n",
    "    # Clean up infinities\n",
    "    ses_agg.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "else:\n",
    "    ses_agg = pd.DataFrame(columns=[\"user_id\"])  # empty (edge case)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77d0fa4-8dca-4028-819d-cf718e6d3b3e",
   "metadata": {},
   "source": [
    "## Events (feature adoption features)\n",
    "* Product usage logs → aggregated into unique_features_used, success_rate, days_to_first_event.\n",
    "* Captures what the user is doing inside the product and how effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "84c38960-c88e-4c18-8027-274c52a6c6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(events) > 0:\n",
    "    ev = events.copy()\n",
    "    ev_agg = ev.groupby(\"user_id\").agg(\n",
    "        total_events=(\"event_id\", \"nunique\"),\n",
    "        unique_features_used=(\"feature_name\", pd.Series.nunique),\n",
    "        success_rate=(\"success\", \"mean\") if \"success\" in ev.columns else (\"event_id\", \"count\"),\n",
    "        avg_latency_ms=(\"latency_ms\", \"mean\") if \"latency_ms\" in ev.columns else (\"event_id\", \"count\"),\n",
    "        first_event=(\"ts\", \"min\"),\n",
    "        last_event=(\"ts\", \"max\")\n",
    "    )\n",
    "\n",
    "    # time to first feature use since signup (days)\n",
    "    if \"first_event\" in ev_agg.columns and \"signup_date\" in users.columns:\n",
    "        ev_agg = ev_agg.merge(users[[\"user_id\", \"signup_date\"]], on=\"user_id\", how=\"left\")\n",
    "        ev_agg[\"days_to_first_event\"] = (ev_agg[\"first_event\"] - ev_agg[\"signup_date\"]).dt.days\n",
    "        ev_agg.drop(columns=[\"signup_date\"], inplace=True, errors=\"ignore\")\n",
    "else:\n",
    "    ev_agg = pd.DataFrame(columns=[\"user_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40acef0e-1a9b-4e02-a77b-fc8b079ed8a6",
   "metadata": {},
   "source": [
    "## Billing (financial health features)\n",
    "* Invoices and payments → aggregated into avg_mrr, ever_overdue, total_tickets\n",
    "* Captures the business relationship health (financial reliability, support burden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6bec21a2-840b-4683-aecb-5866a1a07ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 Billing-level features (financial health)\n",
    "if len(billing) > 0:\n",
    "    bill = billing.copy()\n",
    "    if \"month\" in bill.columns:\n",
    "        # parse month if provided as string\n",
    "        bill[\"month\"] = pd.to_datetime(bill[\"month\"], errors=\"coerce\")\n",
    "\n",
    "    bill_agg = bill.groupby(\"user_id\").agg(\n",
    "        avg_mrr=(\"mrr\", \"mean\") if \"mrr\" in bill.columns else (\"user_id\", \"size\"),\n",
    "        max_mrr=(\"mrr\", \"max\") if \"mrr\" in bill.columns else (\"user_id\", \"size\"),\n",
    "        min_mrr=(\"mrr\", \"min\") if \"mrr\" in bill.columns else (\"user_id\", \"size\"),\n",
    "        months_billed=(\"month\", \"nunique\") if \"month\" in bill.columns else (\"user_id\", \"size\"),\n",
    "        ever_overdue=(\"invoices_overdue\", \"max\") if \"invoices_overdue\" in bill.columns else (\"user_id\", \"size\"),\n",
    "        total_tickets=(\"support_ticket_count\", \"sum\") if \"support_ticket_count\" in bill.columns else (\"user_id\", \"size\"),\n",
    "        discount_ever=(\"discount_applied\", \"max\") if \"discount_applied\" in bill.columns else (\"user_id\", \"size\"),\n",
    "        active_seats_max=(\"active_seats\", \"max\") if \"active_seats\" in bill.columns else (\"user_id\", \"size\"),\n",
    "    )\n",
    "else:\n",
    "    bill_agg = pd.DataFrame(columns=[\"user_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6356b6-85be-404d-a1d1-ec7bf8d31112",
   "metadata": {},
   "source": [
    "# Merging\n",
    "* Feature engineering gets combined into a single dataset to feed into a machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a2b0bfc8-382a-4e96-bf68-65a06612a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = user_feats.merge(ses_agg.reset_index(), on=\"user_id\", how=\"left\")\n",
    "df = df.merge(ev_agg.reset_index(), on=\"user_id\", how=\"left\")\n",
    "df = df.merge(bill_agg.reset_index(), on=\"user_id\", how=\"left\")\n",
    "\n",
    "# Target\n",
    "y = df[label_col].astype(int)\n",
    "\n",
    "# Drop identifiers / leakage columns\n",
    "drop_cols = [\"user_id\", label_col, \"first_session\", \"last_session\", \"first_event\", \"last_event\"]\n",
    "X = df.drop(columns=[c for c in drop_cols if c in df.columns], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d15ab8-c83e-45c2-abef-2472e449125a",
   "metadata": {},
   "source": [
    "# Train / Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fa8d8da5-f5e1-4ef5-bf64-f62882b3c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, stratify=y, random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0ae87f-4aff-4567-bd35-af0b2868f353",
   "metadata": {},
   "source": [
    "# Preprocess\n",
    "* Prepare features (X) so they’re in a form that machine learning models can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a56bfa22-dc7c-40d5-9a9c-e0641cc0a8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits features into categorical (e.g., plan_tier, region) and numeric (e.g., avg_session_length, account_age_days)\n",
    "# Models like logistic regression can’t handle raw strings → we need to encode them\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "num_cols = X_train.select_dtypes(include=[\"number\", \"bool\"]).columns.tolist()\n",
    "\n",
    "# Imputer: fills in missing values with the median (robust against outliers)\n",
    "# Scaler: normalizes numeric features (so values like 1000 MRR and 0.3 success rate are on comparable scales)\n",
    "numeric_tf = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler(with_mean=False))  # with_mean False to be safe with sparse concat\n",
    "])\n",
    "\n",
    "# Imputer: fills missing categories with the most common one.\n",
    "# OneHotEncoder: turns categories into binary columns.\n",
    "# Example: plan_tier = Premium → [Free=0, Standard=0, Premium=1, Enterprise=0].\n",
    "# handle_unknown=\"ignore\": prevents errors if a new category shows up at test time.\n",
    "categorical_tf = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True))\n",
    "])\n",
    "\n",
    "# Combines the numeric and categorical pipelines.\n",
    "# Applies each transformation to the right columns.\n",
    "# Now, every row becomes a fully numeric feature vector ready for modeling.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_tf, num_cols),\n",
    "        (\"cat\", categorical_tf, cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3befb0-ecab-4d75-80a6-2548c1cd4998",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
